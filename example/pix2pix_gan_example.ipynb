{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix_gan_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeotaF3G5YLtcsGGKi7aoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayne0git/tensorflow_basic/blob/main/example/pix2pix_gan_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2Pix GAN for Image Translation\n",
        "Ref - https://pyimagesearch.com/2022/07/27/image-translation-with-pix2pix/"
      ],
      "metadata": {
        "id": "p0qwnt3wqgrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library"
      ],
      "metadata": {
        "id": "Q81Npl9aqwyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import os"
      ],
      "metadata": {
        "id": "0-ycN3bcqykI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.pyplot import subplots"
      ],
      "metadata": {
        "id": "3oFkRbCQ2Z9h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Input\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.utils import get_file"
      ],
      "metadata": {
        "id": "N1ghnO9RrPB7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter"
      ],
      "metadata": {
        "id": "9iQG3Emjqppw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# name of the dataset we will be using \n",
        "DATASET = \"cityscapes\"\n",
        "\n",
        "# build the dataset URL\n",
        "DATASET_URL = f\"http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{DATASET}.tar.gz\"\n",
        "\n",
        "# dataset specs\n",
        "IMAGE_WIDTH = 256\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_CHANNELS = 3"
      ],
      "metadata": {
        "id": "KB0M5Bo5q9iC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the batch size\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "INFER_BATCH_SIZE = 8\n",
        "\n",
        "# training specs\n",
        "LEARNING_RATE = 2e-4\n",
        "EPOCHS = 1\n",
        "STEPS_PER_EPOCH = 100"
      ],
      "metadata": {
        "id": "67fT0roVrC2p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-bt9Z1u8qdGW"
      },
      "outputs": [],
      "source": [
        "# path to our base output directory\n",
        "BASE_OUTPUT_PATH = \"outputs\"\n",
        "\n",
        "# GPU training pix2pix model paths\n",
        "GENERATOR_MODEL = os.path.join(BASE_OUTPUT_PATH, \"models\", \"generator\")\n",
        "\n",
        "# define the path to the inferred images and to the grid image\n",
        "BASE_IMAGES_PATH = os.path.join(BASE_OUTPUT_PATH, \"images\")\n",
        "GRID_IMAGE_PATH = os.path.join(BASE_IMAGES_PATH, \"grid.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "2XVNdcndrRCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(imageFile):\n",
        "\t# read and decode an image file from the path\n",
        "\timage = tf.io.read_file(imageFile)\n",
        "\timage = tf.io.decode_jpeg(image, channels=3)\n",
        "\n",
        "\t# calculate the midpoint of the width and split the combined image into input mask and real image \n",
        "\twidth = tf.shape(image)[1]\n",
        "\tsplitPoint = width // 2\n",
        "\tinputMask = image[:, splitPoint:, :]\n",
        "\trealImage = image[:, :splitPoint, :]\n",
        "\n",
        "\t# convert both images to float32 tensors and convert pixels to the range of -1 and 1\n",
        "\tinputMask = tf.cast(inputMask, tf.float32)/127.5 - 1\n",
        "\trealImage = tf.cast(realImage, tf.float32)/127.5 - 1\n",
        "\n",
        "\t# return the input mask and real label image\n",
        "\treturn (inputMask, realImage)"
      ],
      "metadata": {
        "id": "3mdOj1vNrSyX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_jitter(inputMask, realImage, height, width):\n",
        "\t# upscale the images for cropping purposes\n",
        "\tinputMask = tf.image.resize(inputMask, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\trealImage = tf.image.resize(realImage, [height, width],\tmethod=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "\t# return the input mask and real label image\n",
        "\treturn (inputMask, realImage)"
      ],
      "metadata": {
        "id": "i04ZLsc3roWC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadTrainExample(object):\n",
        "\tdef __init__(self, imageHeight, imageWidth):\n",
        "\t\tself.imageHeight = imageHeight\n",
        "\t\tself.imageWidth = imageWidth\n",
        "\t\n",
        "\tdef __call__(self, imageFile):\n",
        "\t\t# read the file path and unpack the image pair\n",
        "\t\tinputMask, realImage = load_image(imageFile)\n",
        "\n",
        "\t\t# perform data augmentation\n",
        "\t\t# upscale the image and add random artifacts to our image \n",
        "\t\t(inputMask, realImage) = random_jitter(inputMask, realImage, self.imageHeight+30, self.imageWidth+30)\n",
        "\n",
        "\t\t# reshape the input mask and real label image\n",
        "\t\tinputMask = tf.image.resize(inputMask, [self.imageHeight, self.imageWidth])\n",
        "\t\trealImage = tf.image.resize(realImage, [self.imageHeight, self.imageWidth])\n",
        "\n",
        "\t\t# return the input mask and real label image\n",
        "\t\treturn (inputMask, realImage)"
      ],
      "metadata": {
        "id": "yndV082ir2xR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadTestExample(object):\n",
        "\tdef __init__(self, imageHeight, imageWidth):\n",
        "\t\tself.imageHeight = imageHeight\n",
        "\t\tself.imageWidth = imageWidth\n",
        "\n",
        "\tdef __call__(self, imageFile):\n",
        "\t\t# read the file path and unpack the image pair\n",
        "\t\t(inputMask, realImage) = load_image(imageFile)\n",
        "\n",
        "\t\t# reshape the input mask and real label image\n",
        "\t\tinputMask = tf.image.resize(inputMask, [self.imageHeight, self.imageWidth])\n",
        "\t\trealImage = tf.image.resize(realImage, [self.imageHeight, self.imageWidth])\n",
        "\n",
        "\t\t# return the input mask and real label image\n",
        "\t\treturn (inputMask, realImage)"
      ],
      "metadata": {
        "id": "J89ecMp7wwc8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, batchSize, height, width, train=False):\n",
        "\t# check if this is the training dataset\n",
        "\tif train:\n",
        "\t\tdataset = tf.data.Dataset.list_files(str(path/\"train/*.jpg\"))\n",
        "\t\tdataset = dataset.map(ReadTrainExample(height, width), num_parallel_calls=AUTO)\n",
        "\t# otherwise, we are working with the test dataset\n",
        "\telse:\n",
        "\t\tdataset = tf.data.Dataset.list_files(str(path/\"val/*.jpg\"))\n",
        "\t\tdataset = dataset.map(ReadTestExample(height, width), num_parallel_calls=AUTO)\n",
        "\n",
        "\t# shuffle, batch, repeat and prefetch the dataset\n",
        "\tdataset = dataset.shuffle(batchSize * 2).batch(batchSize).repeat().prefetch(AUTO)\n",
        "\n",
        "\t# return the dataset\n",
        "\treturn dataset"
      ],
      "metadata": {
        "id": "TPTfgAjbspKG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the cityscape training dataset \n",
        "pathToZip = get_file(fname=f\"{DATASET}.tar.gz\", origin=DATASET_URL,\textract=True)\n",
        "pathToZip  = pathlib.Path(pathToZip)\n",
        "path = pathToZip.parent/DATASET"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCesqrpNzAU8",
        "outputId": "f4ffc574-6825-46c9-ba1c-7a5af7a912d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/cityscapes.tar.gz\n",
            "103448576/103441232 [==============================] - 27s 0us/step\n",
            "103456768/103441232 [==============================] - 27s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset\n",
        "trainDs = load_dataset(path=path, train=True, batchSize=TRAIN_BATCH_SIZE, height=IMAGE_HEIGHT, width=IMAGE_WIDTH)\n",
        "testDs = load_dataset(path=path, train=False, batchSize=INFER_BATCH_SIZE, height=IMAGE_HEIGHT, width=IMAGE_WIDTH)"
      ],
      "metadata": {
        "id": "2XBRZj71zcvS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "tGKo6vZUvD7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2Pix(object):\n",
        "\tdef __init__(self, imageHeight, imageWidth):\n",
        "\t\tself.imageHeight = imageHeight\n",
        "\t\tself.imageWidth = imageWidth\n",
        "\n",
        "\tdef generator(self):\n",
        "\t\t# initialize the input layer (256*256*3)\n",
        "\t\tinputs = Input([self.imageHeight, self.imageWidth, 3])\n",
        "  \n",
        "\t\t# down Layer 1 (d1) => final layer 1 (f1) (128*128*32)\n",
        "\t\td1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
        "\t\td1 = Dropout(0.1)(d1)\n",
        "\t\tf1 = MaxPool2D((2, 2))(d1)\n",
        "\n",
        "\t\t# down Layer 2 (l2) => final layer 2 (f2) (64*64*64)\n",
        "\t\td2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(f1)\n",
        "\t\tf2 = MaxPool2D((2, 2))(d2)\n",
        "\n",
        "\t\t#  down Layer 3 (l3) => final layer 3 (f3) (32*32*96)\n",
        "\t\td3 = Conv2D(96, (3, 3), activation=\"relu\", padding=\"same\")(f2)\n",
        "\t\tf3 = MaxPool2D((2, 2))(d3)\n",
        "\n",
        "\t\t# down Layer 4 (l3) => final layer 4 (f4) (16*16*96)\n",
        "\t\td4 = Conv2D(96, (3, 3), activation=\"relu\", padding=\"same\")(f3)\n",
        "\t\tf4 = MaxPool2D((2, 2))(d4)\n",
        "\n",
        "\t\t# u-bend of the u-bet (16*16*256)\n",
        "\t\tb5 = Conv2D(96, (3, 3), activation=\"relu\", padding=\"same\")(f4)\n",
        "\t\tb5 = Dropout(0.3)(b5)\n",
        "\t\tb5 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(b5)\n",
        "\n",
        "\t\t# upsample Layer 6 (u6) (32*32*128)\n",
        "\t\tu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding=\"same\")(b5)\n",
        "\t\tu6 = concatenate([u6, d4])\n",
        "\t\tu6 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(u6)\n",
        "\n",
        "\t\t# upsample Layer 7 (u7) (64*64*128)\n",
        "\t\tu7 = Conv2DTranspose(96, (2, 2), strides=(2, 2), padding=\"same\")(u6)\n",
        "\t\tu7 = concatenate([u7, d3])\n",
        "\t\tu7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(u7)\n",
        "\n",
        "\t\t# upsample Layer 8 (u8) (128*128*128)\n",
        "\t\tu8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(u7)\n",
        "\t\tu8 = concatenate([u8, d2])\n",
        "\t\tu8 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(u8)\n",
        "\n",
        "\t\t# upsample Layer 9 (u9) (256*256*128)\n",
        "\t\tu9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\"same\")(u8)\n",
        "\t\tu9 = concatenate([u9, d1])\n",
        "\t\tu9 = Dropout(0.1)(u9)\n",
        "\t\tu9 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(u9)\n",
        "\n",
        "\t\t# final conv2D layer (256*256*3)\n",
        "\t\toutputLayer = Conv2D(3, (1, 1), activation=\"tanh\")(u9)\n",
        "\t\n",
        "\t\t# create the generator model\n",
        "\t\tgenerator = Model(inputs, outputLayer)\n",
        "\n",
        "\t\treturn generator\n",
        "\n",
        "\tdef discriminator(self):\n",
        "\t\t# initialize input layer according to PatchGAN\n",
        "\t\tinputMask = Input(shape=[self.imageHeight, self.imageWidth, 3], name=\"input_image\")\n",
        "\t\ttargetImage = Input(shape=[self.imageHeight, self.imageWidth, 3], name=\"target_image\")\n",
        "  \n",
        "\t\t# concatenate the inputs (256*256*6)\n",
        "\t\tx = concatenate([inputMask, targetImage])  \n",
        "\n",
        "\t\t# add four conv2D convolution layers\n",
        "\t\tx = Conv2D(64, 4, strides=2, padding=\"same\")(x)  # (128*128*64)\n",
        "\t\tx = LeakyReLU()(x)\n",
        "\n",
        "\t\tx = Conv2D(128, 4, strides=2, padding=\"same\")(x)  # (64*64*128)\n",
        "\t\tx = LeakyReLU()(x)\n",
        "\n",
        "\t\tx = Conv2D(256, 4, strides=2, padding=\"same\")(x)  # (32*32*256)\n",
        "\t\tx = LeakyReLU()(x)\n",
        "\n",
        "\t\tx = Conv2D(512, 4, strides=1, padding=\"same\")(x)  # (32*32*512)\n",
        "\n",
        "\t\t# add a batch-normalization layer => LeakyReLU => zeropad\n",
        "\t\tx = BatchNormalization()(x)\n",
        "\t\tx = LeakyReLU()(x)\n",
        "\n",
        "\t\t# final conv layer (30*30*1)\n",
        "\t\tlast = Conv2D(1, 3, strides=1)(x)\n",
        "  \n",
        "\t\t# create the discriminator model\n",
        "\t\tdiscriminator = Model(inputs=[inputMask, targetImage], outputs=last)\n",
        "\n",
        "\t\treturn discriminator"
      ],
      "metadata": {
        "id": "-6HoZ_p8vGFK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the generator and discriminator network\n",
        "pix2pixObject = Pix2Pix(imageHeight=IMAGE_HEIGHT, imageWidth=IMAGE_WIDTH)\n",
        "generator = pix2pixObject.generator()\n",
        "discriminator = pix2pixObject.discriminator()"
      ],
      "metadata": {
        "id": "9Xt2cWyrzxjJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "22G-mPfDwWfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2PixTraining(Model):\n",
        "\tdef __init__(self, generator, discriminator):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.generator = generator\n",
        "\t\tself.discriminator = discriminator\n",
        "\n",
        "\tdef compile(self, gOptimizer, dOptimizer, bceLoss, maeLoss):\n",
        "\t\tsuper().compile()\n",
        "\n",
        "\t\t# initialize the optimizers for the generator and discriminator\n",
        "\t\tself.gOptimizer = gOptimizer\n",
        "\t\tself.dOptimizer = dOptimizer\n",
        "\t\t\n",
        "\t\t# initialize the loss functions\n",
        "\t\tself.bceLoss = bceLoss\n",
        "\t\tself.maeLoss = maeLoss\n",
        "\n",
        "\tdef train_step(self, inputs):\n",
        "\t\t# grab the input mask and corresponding real images\n",
        "\t\t(inputMask, realImages) = inputs\n",
        "\n",
        "\t\t# initialize gradient tapes for both generator and discriminator\n",
        "\t\twith tf.GradientTape() as genTape, tf.GradientTape() as discTape:\n",
        "\t\t\t# generate fake images\n",
        "\t\t\tfakeImages = self.generator(inputMask, training=True)\n",
        "\n",
        "\t\t\t# discriminator output for real images and fake images\n",
        "\t\t\tdiscRealOutput = self.discriminator([inputMask, realImages], training=True)\n",
        "\t\t\tdiscFakeOutput = self.discriminator([inputMask, fakeImages], training=True)\n",
        "\n",
        "\t\t\t# compute the adversarial loss for the generator\n",
        "\t\t\tmisleadingImageLabels = tf.ones_like(discFakeOutput) \n",
        "\t\t\tganLoss = self.bceLoss(misleadingImageLabels, discFakeOutput)\n",
        "\n",
        "\t\t\t# compute the mean absolute error between the fake and the real images\n",
        "\t\t\tl1Loss = self.maeLoss(realImages, fakeImages)\n",
        "\n",
        "\t\t\t# compute the total generator loss\n",
        "\t\t\ttotalGenLoss = ganLoss + (10 * l1Loss)\n",
        "\n",
        "\t\t\t# discriminator loss for real and fake images\n",
        "\t\t\trealImageLabels = tf.ones_like(discRealOutput)\n",
        "\t\t\trealDiscLoss = self.bceLoss(realImageLabels, discRealOutput)\n",
        "\n",
        "\t\t\tfakeImageLabels = tf.zeros_like(discFakeOutput)\n",
        "\t\t\tgeneratedLoss = self.bceLoss(fakeImageLabels, discFakeOutput)\n",
        "\n",
        "\t\t\t# compute the total discriminator loss\n",
        "\t\t\ttotalDiscLoss = realDiscLoss + generatedLoss\n",
        "\n",
        "\t\t# calculate the generator and discriminator gradients\n",
        "\t\tgeneratorGradients = genTape.gradient(totalGenLoss, self.generator.trainable_variables)\n",
        "\t\tdiscriminatorGradients = discTape.gradient(totalDiscLoss, self.discriminator.trainable_variables)\n",
        "\n",
        "\t\t# apply the gradients to optimize the generator and discriminator\n",
        "\t\tself.gOptimizer.apply_gradients(zip(generatorGradients, self.generator.trainable_variables))\n",
        "\t\tself.dOptimizer.apply_gradients(zip(discriminatorGradients, self.discriminator.trainable_variables))\n",
        "\n",
        "\t\t# return the generator and discriminator losses\n",
        "\t\treturn {\"dLoss\": totalDiscLoss, \"gLoss\": totalGenLoss}"
      ],
      "metadata": {
        "id": "IpHGs1KEwXew"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_monitor(testDs, imagePath, batchSize, epochInterval):\n",
        "\t# grab the input mask and the real image from the testing dataset\n",
        "\t(tInputMask, tRealImage) = next(iter(testDs))\n",
        "\n",
        "\tclass TrainMonitor(Callback):\n",
        "\t\tdef __init__(self, epochInterval=None):\n",
        "\t\t\tself.epochInterval = epochInterval\n",
        "\n",
        "\t\tdef on_epoch_end(self, epoch, logs=None):\n",
        "\t\t\tif self.epochInterval and epoch % self.epochInterval == 0:\n",
        "\t\t\t\t# get the pix2pix prediction\n",
        "\t\t\t\ttPix2pixGenPred = self.model.generator.predict(tInputMask)\n",
        "\t\t\t\t(fig, axes) = subplots(nrows=batchSize, ncols=3, figsize=(50, 50))\n",
        "\n",
        "\t\t\t\t# plot the predicted images \n",
        "\t\t\t\tfor (ax, inp, pred, tgt) in zip(axes, tInputMask, tPix2pixGenPred, tRealImage):\n",
        "\t\t\t\t\t# plot the input mask image\n",
        "\t\t\t\t\tax[0].imshow(array_to_img(inp))\n",
        "\t\t\t\t\tax[0].set_title(\"Input Image\")\n",
        "\n",
        "\t\t\t\t\t# plot the predicted Pix2Pix image\n",
        "\t\t\t\t\tax[1].imshow(array_to_img(pred))\n",
        "\t\t\t\t\tax[1].set_title(\"Pix2Pix Prediction\")\n",
        "\n",
        "\t\t\t\t\t# plot the ground truth\n",
        "\t\t\t\t\tax[2].imshow(array_to_img(tgt))\n",
        "\t\t\t\t\tax[2].set_title(\"Target Label\")\n",
        "\n",
        "\t\t\t\tplt.savefig(f\"{imagePath}/{epoch:03d}.png\")\n",
        "\t\t\t\tplt.close()\n",
        "\t\n",
        "\t# instantiate a train monitor callback\n",
        "\ttrainMonitor = TrainMonitor(epochInterval=epochInterval)\n",
        "\n",
        "\treturn trainMonitor    "
      ],
      "metadata": {
        "id": "nVhz2RbX2iri"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "c4Ga24sOy7f5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the pix2pix model\n",
        "pix2pixModel = Pix2PixTraining(generator=generator,\tdiscriminator=discriminator)\n",
        "pix2pixModel.compile(dOptimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "                     gOptimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "                     bceLoss=BinaryCrossentropy(from_logits=True),\n",
        "                     maeLoss=MeanAbsoluteError())"
      ],
      "metadata": {
        "id": "cLOnFAevy7y9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether output model directory exists\n",
        "if not os.path.exists(BASE_OUTPUT_PATH):\n",
        "\tos.makedirs(BASE_OUTPUT_PATH)"
      ],
      "metadata": {
        "id": "QOXWEzYE1_mh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether output image directory exists, if it doesn't, then create it\n",
        "if not os.path.exists(BASE_IMAGES_PATH):\n",
        "\tos.makedirs(BASE_IMAGES_PATH)"
      ],
      "metadata": {
        "id": "K9CA3xeS2ENb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the pix2pix model\n",
        "callbacks = [get_train_monitor(testDs, epochInterval=10, imagePath=BASE_IMAGES_PATH, batchSize=INFER_BATCH_SIZE)]\n",
        "pix2pixModel.fit(trainDs, epochs=EPOCHS, callbacks=callbacks, steps_per_epoch=STEPS_PER_EPOCH)"
      ],
      "metadata": {
        "id": "bIhcUsvS2Hnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the pix2pix generator\n",
        "pix2pixModel.generator.save(GENERATOR_MODEL)"
      ],
      "metadata": {
        "id": "rTIZtrCl3TNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}