{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_3_4_Colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayne0git/tensorflow_basic/blob/main/tflite/tflite_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJruOcIiUKel"
      },
      "source": [
        "## Tensorflow Lite Basics\n",
        "Ref - https://learning.edx.org/course/course-v1:HarvardX+TinyML2+3T2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6k2Pg0gTYB8"
      },
      "source": [
        "import tensorflow as tf # 2.8.0\n",
        "import numpy as np   # 1.21.6"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-wlpRp_UZDm"
      },
      "source": [
        "### Prepare simple Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAwdHf6ySQGt"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Linear regression model\n",
        "model = Sequential([Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# Train\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsa3r3UwYGXw"
      },
      "source": [
        "# Check result\n",
        "print(model.predict([10.0]))\n",
        "print(\"Here is what I learned: {}\".format(model.layers[0].get_weights()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOs_IDM6ToaM"
      },
      "source": [
        "# Save Tensorflow Model\n",
        "# *.pb / assets / variables\n",
        "export_dir = 'saved_model/1'\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08HjCdHuVx_n"
      },
      "source": [
        "### Tensorflow Lite Converter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWSlkprhTsWE"
      },
      "source": [
        "# Init the converter (*.pb)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# # Init the converter (keras model)\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5tjRxmLY2G_"
      },
      "source": [
        "# 1. Convert Options (No Optimization)\n",
        "# converter.optimizations = []\n",
        "\n",
        "# 2. Convert Options (Default : 8-bit weight quantization)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Float 16 quantization (Weight only)\n",
        "# converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# 8-bit quantization (Weight & Activation)\n",
        "def representative_data_gen():\n",
        "    yield [np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=np.float32)]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8"
      ],
      "metadata": {
        "id": "BIoH6pcchxjx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAEPkAPYZCjU"
      },
      "source": [
        "# Convert the model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsaEjJfrTujk"
      },
      "source": [
        "# Save the TFLite model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNSvtDj7Wg1X"
      },
      "source": [
        "### Tensorflow Lite Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PguT-zY3W50o"
      },
      "source": [
        "# Load TFLite model (From File)\n",
        "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
        "\n",
        "# Load TFLite model (From convert model)\n",
        "# interpreter = tf.lite.Interpreter(model_content=tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fseX4pkhTzS0"
      },
      "source": [
        "# Allocate tensors.\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emi5aWGrXJdP",
        "outputId": "d9b00466-1f3c-4146-f3f7-8e5e54c9e189"
      },
      "source": [
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details) # list[dict]\n",
        "print(output_details) # list[dict]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'serving_default_dense_1_input:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall:0', 'index': 3, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Ij8_BvU0KV"
      },
      "source": [
        "# Inference\n",
        "interpreter.set_tensor(input_details[0]['index'], np.array([[10.0]], dtype=np.float32))\n",
        "interpreter.invoke()\n",
        "interpreter.get_tensor(output_details[0]['index'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}